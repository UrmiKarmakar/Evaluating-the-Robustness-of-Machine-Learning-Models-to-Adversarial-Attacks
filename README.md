# Evaluating-the-Robustness-of-Machine-Learning-Models-to-Adversarial-Attacks
Evaluating the robustness of machine learning models against adversarial attacks is crucial for ensuring their reliability in real-world applications. This project conducts a comprehensive analysis of adversarial perturbations using the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) on the MNIST dataset. It compares the performance of Logistic Regression and Convolutional Neural Networks (CNNs) under adversarial conditions and explores dimensionality reduction techniques such as Principal Component Analysis (PCA) to enhance computational efficiency. The study further examines defense mechanisms, including adversarial training and gradient masking, to mitigate the impact of adversarial attacks. By highlighting model vulnerabilities and evaluating potential countermeasures, this research contributes to the ongoing efforts to improve the security and reliability of deep learning models in adversarial environments.
